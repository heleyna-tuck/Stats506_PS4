---
title: "Stats506_PS4"
author: "Heleyna Tucker"
format:
  html:
    embed-resources: true
editor: visual
---

# Problem 1 - Tidyverse

I will use the tidyverse for this problem. In particular, I will use piping and dplyr as much as I am able.

Install and load the nycflights13, tidyverse, and dplyr packages below:

```{r}
library(tidyverse)
library(nycflights13)
library(dplyr)
```

### Part A: Generating summary tables for the data

Below I will generate a table (which can just be a nicely printed tibble) reporting the mean and median departure delay per airport. I will generate a second table (which can also be printed as a tibble) reporting the mean and median arrival delay per airport. Exclude any destination with under 10 flights. Do this exclusion through code, not manually.

Additionally, I will:

-   Order both tables in descending mean delay.

-   Both tables should use the airport *names* not the airport *codes*.

-   Both tables should print all rows.

Visualize the data below:

```{r}
head(airports)
head(flights)
head(planes)
```

Below, using tidyverse pipping techniques, I will first find the mean and median of dep_delay in the flights data set. I will then order the flights in descending mean order. Using left_join() I will access the name of the airports from the airports data set.

```{r}
dep_delays <- flights %>%
  group_by(origin) %>%
  summarize(
    mean_delay_dep = mean(dep_delay, na.rm = TRUE),
    median_delay_dep = median(dep_delay, na.rm = TRUE),
  ) %>%
  arrange(desc(mean_delay_dep)) %>%
  left_join(airports, by = c("origin" = "faa")) %>%
  select(name, mean_delay_dep, median_delay_dep)
print(dep_delays)
```

Below I use the same technique as above to order and find the mean and median for the arrival airport. I will also exclude any destination that has under 10 flights using a count variable and filter() to filter out any flights who have a flight count less than or equal to 10:

```{r}
arrival_delays <- flights %>%
  group_by(dest) %>%
  summarize(
    mean_delay_arr = mean(dep_delay, na.rm = TRUE),
    median_delay_arr = median(dep_delay, na.rm = TRUE),
    count = n()
  ) %>%
  filter(count >= 10) %>%
  arrange(desc(mean_delay_arr)) %>%
  left_join(airports, by = c("dest" = "faa")) %>%
  select(name, mean_delay_arr, median_delay_arr)
print(arrival_delays)
```

### Part B: How many flights did the aircraft model with the fastest average speed take?

Below I will answer the question and produce a tibble with 1 row, and entries for the model, average speed (in MPH) and number of flights.

First, I will calculate the average speed in MPH for each aircraft below and make a new variable flights_speed with these averages:

Because I need to access the model of each plane, I will use a left_join to join the flights and planes data with the tailnum variable, group by the model and calculate the average distance for each model and the number of flights each model has. This is shown below:

```{r}
flights_speed <- flights %>%
  left_join(planes, by = "tailnum") %>%
  group_by(model) %>%
  summarize(avg_speed_mph = sum(distance)/ sum(air_time), 
            n_flights = n()) %>%
  arrange(desc(avg_speed_mph))
```

Get the aircraft with the fastest average speed:

```{r}
fastest <- flights_speed[1,]
fastest
```

Above we can see that the plane with the fastest average flights speed is model 777-222 and it has 4 total flights.

# Problem 2 - get_temp()

For this problem, I will use the tidyverse. In particular, I will use piping and dplyr as much as I am able.

Below I will load the Chicago NNMAPS data we used in the visualization lectures. I will write a function get_temp() that allows a user to request the average temperature for a given month. The arguments should be:

-   month: Month, either a numeric 1-12 or a string.

-   year: A numeric year.

-   data: the data set to obtain data from.

-   celsius: logically indicating whether the results should be in Celsius. Default FALSE.

-   average_fn: A function with which to compute the mean. Default is mean.

The output should be a numeric vector of length 1. The code inside the function should, as with the rest of this problem, use the tidyverse. Be sure to sanitize the input.

load in the Chicago NMMAPS data below from the dlnm library. Visualize data:

```{r}
nnmaps <- read_csv('/Users/19892/OneDrive/Documents/STATS506/ProblemSets/Stats506_PS4/chicago-nmmaps.csv')
head(nnmaps)
```

I will the write the get_temp() function below. One thing to not is that I changed month and year to month_in and year_in so there is no overlap with the initial dataset variable names. This was indicated by the professor to be okay:

```{r}
#' Function to calculate the average temperature over a given year and month in a dataset.
#'
#' @param month_in Input month. Can give a numeric 1-12, full length month name or abriviated 3 character month.
#' @param year_in Input year. Can only be from 1997-2000 (The max and min year of the dataset)
#' @param data input dataset, in this case it is nnmaps
#' @param celsius boolean to say whether the user wants the result output to be in fahrenheit (FALSE) or celsius (TRUE)
#' @param average_fn average function to use indicated by user but mean function is used by default
#'
#' @return result - average temperature
#' @export
#'
#' @examples
get_temp <- function(month_in, year_in, data, celsius = FALSE, average_fn = mean){
  
  # Validate and sanitize the input
  if (!is.numeric(year_in) || !year_in %in% data$year){
    stop("Invalid year. Please provide a numeric year between 1997 and 2000.")
  }
  if (is.character(month_in) && !substr(tolower(month_in),1,3) %in% substr(tolower(month.name), 1, 3)){
    stop("Invalid month. Please provide a valid numeric value (1-12) or a month name.")
  }
  else if (!is.character(month_in) && !month_in %in% 1:12){
        stop("Invalid month. Please provide a valid numeric value (1-12) or a month name.")
  }
  
  #Convert months inputed as a string to a numeric:
  if(is.character(month_in) && substr(tolower(month_in),1,3) %in% substr(tolower(month.name), 1, 3)){
    month_in <- as.numeric(match(substr(tolower(month_in),1,3), substr(tolower(month.name), 1, 3)))
  }
  
 # Filter the data for the specified year and month
  result <- data %>%
    filter(year == year_in, month_numeric == month_in) %>%
    select(temp) %>%
    pull() %>%
    average_fn()
  
  # Convert to Celsius if necessary
  if (celsius) {
    result <- (result - 32) * 5/9
  }
  
  return(result)
}
```

Test the function below. As we can see, it is working as we would expect:

```{r}
get_temp("Apr", 1999, data = nnmaps)
```

```{r}
get_temp("Apr", 1999, data = nnmaps, celsius = TRUE)
```

```{r}
get_temp(10, 1998, data = nnmaps, average_fn = median)
```

```{r}
get_temp(13, 1998, data = nnmaps)
```

```{r}
get_temp(2, 2005, data = nnmaps)
```

```{r}
get_temp("November", 1999, data =nnmaps, celsius = TRUE,
         average_fn = function(x) {
           x %>% sort -> x
           x[2:(length(x) - 1)] %>% mean %>% return
         })
```

# Problem 3 - SAS

This problem will be done entirely within SAS.

Access the RECS 2020 data and download a copy of the data. Load or import the data into SAS. I will answer the following questions:

### Part A: What state has the highest percentage of records?

What state has the highest percentage of records? What percentage of all records correspond to Michigan? (Don't forget to account for the sampling weights!)

The code that I used for this question can be found below:

```         
/* data libraries for reading/writing data: -------------------------------- */
%let in_path = ~/sasuser.v94/input_data;
%let out_path = ~/sasuser.v94/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data recs; 
 set in_lib.recs2020_public_v5;

proc freq data = recs;
	TABLES state_name / OUT = StateFreq;
	WEIGHT NWEIGHT;
RUN;

proc sort data = StateFreq;
	BY DESCENDING COUNT;
RUN;

DATA MichiganPercentage;
	SET StateFreq;
	WHERE state_name = 'Michigan';
RUN;
```

Above loads in the data as recs, then gets the frequency of the state records, taking into account the weight of NWEIGHT. Then, proc sort sorts the data into descending order by the count/frequency of records to obtain the highest percentage of records. This was found to be California The next code block finds the percentage corresponding to Michigan (you could look at the original frequency table and find Michigan). This was found to be about 3.17%

### Part B: Generate histogram (total electricity cost)

Generate a histogram of the total electricity cost in dollars, amongst those with a strictly positive cost.

Below is the code I used for this part, using the variable DOLLAREL for the total electricity cost in dollars. I used the WHERE command to get all the strictly positive values.

```         
PROC SGPLOT DATA=recs;
   HISTOGRAM DOLLAREL / 
      FILLATTRMAP=GraphDataAttrMap;
   WHERE DOLLAREL > 0; /* Filter for strictly positive costs */
   XAXIS LABEL="Total Electricity Cost (Dollars)";
   YAXIS LABEL="Frequency";
RUN;
```

When we run the above code, we can see from the histogram that around 1,000 dollars tends to be the most frequent total electricity cost.

### Part C: Histogram (log total electricity cost)

Generate a histogram of the log of the total electricity cost. Below I will first make a new variable called LogTotalElectricityCost that takes the log of the total electricity cost of the recs data. Then, I will write the same code as in part B to make the histogram

```         
DATA recs;
    SET recs;
    LogTotalElectricityCost = log(DOLLAREL);
RUN;

/* Create a histogram of the log of the total electricity cost */
PROC SGPLOT DATA=recs;
    HISTOGRAM LogTotalElectricityCost / 
        FILLATTRMAP=GraphDataAttrMap;
    WHERE LogTotalElectricityCost > 0; /* Filter for strictly positive log costs */
    XAXIS LABEL="Log of Total Electricity Cost";
    YAXIS LABEL="Frequency";
RUN;
```

The output of this part showed that around 7.5 was the most frequent log of total electricity cost.

### Part D: Fit a linear regression model

Fit a linear regression model predicting the log of the total electricity cost based upon the number of rooms in the house and whether or not the house has a garage. (Don't forget weights)

Below I fit a model predicting the LogTotalElectricityCost (made in Part C) using the total number of rooms (TOTROOMS) and the whether or not the house has a garage (PRKGPLC1):

```         
/* Fit a weighted linear regression model */
PROC REG DATA=recs;
    WEIGHT NWEIGHT; /* Replace with the actual variable name for sampling weights */
    MODEL LogTotalElectricityCost = TOTROOMS PRKGPLC1;
    TITLE "Linear Regression Model for Log of Total Electricity Cost";
RUN;
```

Notably, from the ouput html file attached to the github repo, the predictors both have a significant p-value (\< 0.05). With TOTROOMS having a coefficient of 0.08780 and PRKGLPLC1 having a coefficient of 0.06878.

### Part E: Predicted values and scatterplot

Use the model to generate predicted values and create a scatterplot of predicted total electricity cost vs. actual total electricity cost (not on the log scale).

```         
```

# Problem 4 - Multiple tools

The researcher\'s interest is in whether long-term concerns about climate change impact current day concerns about financial stability. To address this, the particular research question of interest is whether **the respondent\'s family is better off, the same, or worse off finanicially compared to 12 month\'s ago** can be predicted by **thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years**. We also want to control for

-   How they rate the economic conditions today in the country.

-   Whether they own (with or without a mortgage) or rent or neither their home.

-   Education (use the 4-category version)

-   Race (use the 5-category version)

We\'re going to pretend the raw data is extremely large and that we need to extract the subset of the data we\'re going to use before we can open it in Stata or R.

Additionally, the data comes from a complex survey design, so we need to account for that in the analysis.

### Part A: Codebook generation

Take a look at the codebook. For very minor extra credit, how was the Codebook generated? (no loss of points if I decide not to do this)

## SAS

### Part B: Import Data into SAS, use proc sql to slect only the variables you'll need for your analysis, as well as subsetting the data if needed.

Import the data into SAS (you can load the SAS data directly or import the CSV) and use `proc sql` to select only the variables you\'ll need for your analysis, as well as subsetting the data if needed. You can carry out variable transformations now, or save it for Stata.

```         
/* data libraries for reading/writing data: -------------------------------- */
%let in_path = ~/sasuser.v94/input_data;
%let out_path = ~/sasuser.v94/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data public; 
 set in_lib.public2022;
 
 proc sql;
  create table subset_data as
  select
  	CaseID,
  	B3,
  	ND2,
  	ppethm,
  	ppeduc5,
  	GH1,
  	B7_a
  from public
 quit;
```

Above I used select in proc sql to get all the needed variables I will use for my analysis.

### Part C: Get the data out of SAS and into Stata

Get the data out of SAS and into Stata. (Note that this could mean saving the data in SAS format, then importing into Stata; or exporting from SAS into Stata format then loading it in Stata; or exporting from SAS into a generic format and importing into Stata - whichever works for you.)

```         
data out_lib.public_sub;
 set public_sub;
run;
```

Above will make a new file public_sub.sas7bdat that has all the needed variables. This will have to be exported from SAS into Stata format through Stata below:

## Stata

### Part D: Demonstrate that you'be successfully extracted the appropriate data 

Demonstrate that you\'ve successfully extracted the appropriate data by showing the number of observations and variables. (Report these values via Stata code don\'t just say \"As we see in the Properties window\". The Codebook should give you a way to ensure the number of rows is as expected.)

```         
```

### Part E: Convert response to binary

The response variable is a Likert scale; convert it to a binary of worse off versus same/better.

```         
```

### Part F: Logistic regression

Use the following code to tell Stata that the data is from a complex sample:

```         
svyset CaseID [pw=weight_pop]
```

(Modify `CaseID` and `weight_pop` as appropriate if you have different variable names; those names are taken from the Codebook.)

Carry out a logisitic regression model accounting for the complex survey design. Be sure to treat variables you think should be categorical appropriately. From these results, provide an answer to the researchers question of interest.

Notice that the model does not provide a pseudo-Rsq. R has the functionality to do this.

```         
```

### Part G: Get the data out of Stata and into R

```         
```

## R

### Part H: Obtain the pseudo-Rsq value for the logistic model fit above and report it.

Use the survey package to obtain the pseudo-Rsq. Use the following code to set up the complex survey design:

```         
svydesign(id = ~ caseid, weight = ~ weight_pop, data = dat)
```

Obtain the pseudo-Rsq value for the logistic model fit above and report it:

```{r}

```
