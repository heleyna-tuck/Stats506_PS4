---
title: "Stats506_PS4"
author: "Heleyna Tucker"
format:
  html:
    embed-resources: true
editor: visual
---

# Problem 1 - Tidyverse

I will use the tidyverse for this problem. In particular, I will use piping and dplyr as much as I am able.

Install and load the nycflights13, tidyverse, and dplyr packages below:

```{r}
library(tidyverse)
library(nycflights13)
library(dplyr)
```

### Part A: Generating summary tables for the data

Below I will generate a table (which can just be a nicely printed tibble) reporting the mean and median departure delay per airport. I will generate a second table (which can also be printed as a tibble) reporting the mean and median arrival delay per airport. Exclude any destination with under 10 flights. Do this exclusion through code, not manually.

Additionally, I will:

-   Order both tables in descending mean delay.

-   Both tables should use the airport *names* not the airport *codes*.

-   Both tables should print all rows.

Visualize the data below:

```{r}
head(airports)
head(flights)
head(planes)
```

Below, using tidyverse pipping techniques, I will first find the mean and median of dep_delay in the flights data set. I will then order the flights in descending mean order. Using left_join() I will access the name of the airports from the airports data set.

```{r}
dep_delays <- flights %>%
  group_by(origin) %>%
  summarize(
    mean_delay_dep = mean(dep_delay, na.rm = TRUE),
    median_delay_dep = median(dep_delay, na.rm = TRUE),
  ) %>%
  arrange(desc(mean_delay_dep)) %>%
  left_join(airports, by = c("origin" = "faa")) %>%
  select(name, mean_delay_dep, median_delay_dep)
print(dep_delays)
```

Below I use the same technique as above to order and find the mean and median for the arrival airport. I will also exclude any destination that has under 10 flights using a count variable and filter() to filter out any flights who have a flight count less than or equal to 10:

```{r}
arrival_delays <- flights %>%
  group_by(dest) %>%
  summarize(
    mean_delay_arr = mean(dep_delay, na.rm = TRUE),
    median_delay_arr = median(dep_delay, na.rm = TRUE),
    count = n()
  ) %>%
  filter(count >= 10) %>%
  arrange(desc(mean_delay_arr)) %>%
  left_join(airports, by = c("dest" = "faa")) %>%
  select(name, mean_delay_arr, median_delay_arr)
print(arrival_delays)
```

### Part B: How many flights did the aircraft model with the fastest average speed take?

Below I will answer the question and produce a tibble with 1 row, and entries for the model, average speed (in MPH) and number of flights.

First, I will calculate the average speed in MPH for each aircraft below and make a new variable flights_speed with these averages:

Because I need to access the model of each plane, I will use a left_join to join the flights and planes data with the tailnum variable, group by the model and calculate the average distance for each model and the number of flights each model has. This is shown below:

```{r}
flights_speed <- flights %>%
  left_join(planes, by = "tailnum") %>%
  group_by(model) %>%
  summarize(avg_speed_mph = sum(distance)/ sum(air_time), 
            n_flights = n()) %>%
  arrange(desc(avg_speed_mph))
```

Get the aircraft with the fastest average speed:

```{r}
fastest <- flights_speed[1,]
fastest
```

Above we can see that the plane with the fastest average flights speed is model 777-222 and it has 4 total flights.

# Problem 2 - get_temp()

For this problem, I will use the tidyverse. In particular, I will use piping and dplyr as much as I am able.

Below I will load the Chicago NNMAPS data we used in the visualization lectures. I will write a function get_temp() that allows a user to request the average temperature for a given month. The arguments should be:

-   month: Month, either a numeric 1-12 or a string.

-   year: A numeric year.

-   data: the data set to obtain data from.

-   celsius: logically indicating whether the results should be in Celsius. Default FALSE.

-   average_fn: A function with which to compute the mean. Default is mean.

The output should be a numeric vector of length 1. The code inside the function should, as with the rest of this problem, use the tidyverse. Be sure to sanitize the input.

load in the Chicago NMMAPS data below from the dlnm library. Visualize data:

```{r}
nnmaps <- read_csv('/Users/19892/OneDrive/Documents/STATS506/ProblemSets/Stats506_PS4/chicago-nmmaps.csv')
head(nnmaps)
```

I will the write the get_temp() function below. One thing to not is that I changed month and year to month_in and year_in so there is no overlap with the initial dataset variable names. This was indicated by the professor to be okay:

```{r}
#' Function to calculate the average temperature over a given year and month in a dataset.
#'
#' @param month_in Input month. Can give a numeric 1-12, full length month name or abriviated 3 character month.
#' @param year_in Input year. Can only be from 1997-2000 (The max and min year of the dataset)
#' @param data input dataset, in this case it is nnmaps
#' @param celsius boolean to say whether the user wants the result output to be in fahrenheit (FALSE) or celsius (TRUE)
#' @param average_fn average function to use indicated by user but mean function is used by default
#'
#' @return result - average temperature
#' @export
#'
#' @examples
get_temp <- function(month_in, year_in, data, celsius = FALSE, average_fn = mean){
  
  # Validate and sanitize the input
  if (!is.numeric(year_in) || !year_in %in% data$year){
    stop("Invalid year. Please provide a numeric year between 1997 and 2000.")
  }
  if (is.character(month_in) && !substr(tolower(month_in),1,3) %in% substr(tolower(month.name), 1, 3)){
    stop("Invalid month. Please provide a valid numeric value (1-12) or a month name.")
  }
  else if (!is.character(month_in) && !month_in %in% 1:12){
        stop("Invalid month. Please provide a valid numeric value (1-12) or a month name.")
  }
  
  #Convert months inputed as a string to a numeric:
  if(is.character(month_in) && substr(tolower(month_in),1,3) %in% substr(tolower(month.name), 1, 3)){
    month_in <- as.numeric(match(substr(tolower(month_in),1,3), substr(tolower(month.name), 1, 3)))
  }
  
 # Filter the data for the specified year and month
  result <- data %>%
    filter(year == year_in, month_numeric == month_in) %>%
    select(temp) %>%
    pull() %>%
    average_fn()
  
  # Convert to Celsius if necessary
  if (celsius) {
    result <- (result - 32) * 5/9
  }
  
  return(result)
}
```

Test the function below. As we can see, it is working as we would expect:

```{r}
get_temp("Apr", 1999, data = nnmaps)
```

```{r}
get_temp("Apr", 1999, data = nnmaps, celsius = TRUE)
```

```{r}
get_temp(10, 1998, data = nnmaps, average_fn = median)
```

```{r}
#get_temp(13, 1998, data = nnmaps)
#Error:
#Error in get_temp(13, 1998, data = nnmaps) :
#Invalid month. Please provide a valid numeric value (1-12) or a month name.
```

```{r}
#get_temp(2, 2005, data = nnmaps)
#Error:
#Error in get_temp(2, 2005, data = nnmaps) :
#Invalid year. Please provide a numeric year between 1997 and 2000.
```

```{r}
get_temp("November", 1999, data =nnmaps, celsius = TRUE,
         average_fn = function(x) {
           x %>% sort -> x
           x[2:(length(x) - 1)] %>% mean %>% return
         })
```

# Problem 3 - SAS

This problem will be done entirely within SAS.

Access the RECS 2020 data and download a copy of the data. Load or import the data into SAS. I will answer the following questions:

### Part A: What state has the highest percentage of records?

What state has the highest percentage of records? What percentage of all records correspond to Michigan? (Don't forget to account for the sampling weights!)

The code that I used for this question can be found below:

```         
/* data libraries for reading/writing data: -------------------------------- */
%let in_path = ~/sasuser.v94/input_data;
%let out_path = ~/sasuser.v94/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data recs; 
 set in_lib.recs2020_public_v5;

proc freq data = recs;
    TABLES state_name / OUT = StateFreq;
    WEIGHT NWEIGHT;
RUN;

proc sort data = StateFreq;
    BY DESCENDING COUNT;
RUN;

DATA MichiganPercentage;
    SET StateFreq;
    WHERE state_name = 'Michigan';
RUN;
```

Above loads in the data as recs, then gets the frequency of the state records, taking into account the weight of NWEIGHT. Then, proc sort sorts the data into descending order by the count/frequency of records to obtain the highest percentage of records. This was found to be California The next code block finds the percentage corresponding to Michigan (you could look at the original frequency table and find Michigan). This was found to be about 3.17%

### Part B: Generate histogram (total electricity cost)

Generate a histogram of the total electricity cost in dollars, amongst those with a strictly positive cost.

Below is the code I used for this part, using the variable DOLLAREL for the total electricity cost in dollars. I used the WHERE command to get all the strictly positive values.

```         
PROC SGPLOT DATA=recs;
   HISTOGRAM DOLLAREL / 
      FILLATTRMAP=GraphDataAttrMap;
   WHERE DOLLAREL > 0; /* Filter for strictly positive costs */
   XAXIS LABEL="Total Electricity Cost (Dollars)";
   YAXIS LABEL="Frequency";
RUN;
```

When we run the above code, we can see from the histogram that around 1,000 dollars tends to be the most frequent total electricity cost.

### Part C: Histogram (log total electricity cost)

Generate a histogram of the log of the total electricity cost. Below I will first make a new variable called LogTotalElectricityCost that takes the log of the total electricity cost of the recs data. Then, I will write the same code as in part B to make the histogram

```         
DATA recs;
    SET recs;
    LogTotalElectricityCost = log(DOLLAREL);
RUN;

/* Create a histogram of the log of the total electricity cost */
PROC SGPLOT DATA=recs;
    HISTOGRAM LogTotalElectricityCost / 
        FILLATTRMAP=GraphDataAttrMap;
    WHERE LogTotalElectricityCost > 0; /* Filter for strictly positive log costs */
    XAXIS LABEL="Log of Total Electricity Cost";
    YAXIS LABEL="Frequency";
RUN;
```

The output of this part showed that around 7.5 was the most frequent log of total electricity cost.

### Part D: Fit a linear regression model

Fit a linear regression model predicting the log of the total electricity cost based upon the number of rooms in the house and whether or not the house has a garage. (Don't forget weights)

Below I fit a model predicting the LogTotalElectricityCost (made in Part C) using the total number of rooms (TOTROOMS) and the whether or not the house has a garage (PRKGPLC1):

```         
/* Fit a weighted linear regression model */
PROC REG DATA=recs;
    WEIGHT NWEIGHT;
    MODEL LogTotalElectricityCost = TOTROOMS PRKGPLC1;
    TITLE "Linear Regression Model for Log of Total Electricity Cost";
RUN;
```

Notably, from the ouput html file attached to the github repo, the predictors both have a significant p-value (\< 0.05). With TOTROOMS having a coefficient of 0.08780 and PRKGLPLC1 having a coefficient of 0.06878.

### Part E: Predicted values and scatterplot

Use the model to generate predicted values and create a scatterplot of predicted total electricity cost vs. actual total electricity cost (not on the log scale).

```         
/*Obtain the regResults, Predictions of Dollarel and residuals of dollarel from the linear model in part D */
PROC REG DATA=recs;
    WEIGHT NWEIGHT;
    MODEL LogTotalElectricityCost = TOTROOMS PRKGPLC1;
    OUTPUT out=RegResults predicted = PredictedDOLLAREL residual = ResidualDOLLAREL;
    TITLE "Linear Regression Model for Log of Total Electricity Cost";
RUN;

/*Convert from log scale to original*/
DATA RegResults_nolog;
    SET RegResults;
    ActualDOLLAREL = exp(LogTotalElectricityCost);
    PredictedDOLLAREL = exp(PredictedDOLLAREL); /* Convert back to non-log scale */
RUN;

/* Create a scatterplot of predicted vs. actual total electricity cost */
PROC SGPLOT DATA=RegResults_nolog;
    SCATTER X=ActualDOLLAREL Y=PredictedDOLLAREL;
    TITLE "Scatterplot of Predicted vs. Actual Total Electricity Cost";
    XAXIS LABEL="Actual Total Electricity Cost";
    YAXIS LABEL="Predicted Total Electricity Cost";
RUN;
```

From the scatterplot, we can observe some actual and predicted values being quite close to each other, however some points are off. For example, we can see that one data point with an actual value of about 15000 was predicted to have a value of 2500 which is very off.

# Problem 4 - Multiple tools

The researcher's interest is in whether long-term concerns about climate change impact current day concerns about financial stability. To address this, the particular research question of interest is whether **the respondent's family is better off, the same, or worse off finanicially compared to 12 month's ago** can be predicted by **thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years**. We also want to control for

-   How they rate the economic conditions today in the country.

-   Whether they own (with or without a mortgage) or rent or neither their home.

-   Education (use the 4-category version)

-   Race (use the 5-category version)

We're going to pretend the raw data is extremely large and that we need to extract the subset of the data we're going to use before we can open it in Stata or R.

Additionally, the data comes from a complex survey design, so we need to account for that in the analysis.

### Part A: Codebook generation

Take a look at the codebook. For very minor extra credit, how was the Codebook generated? (no loss of points if I decide not to do this)

## SAS

### Part B: Import Data into SAS, use proc sql to slect only the variables you'll need for your analysis, as well as subsetting the data if needed.

Import the data into SAS (you can load the SAS data directly or import the CSV) and use `proc sql` to select only the variables you'll need for your analysis, as well as subsetting the data if needed. You can carry out variable transformations now, or save it for Stata.

```         
/* data libraries for reading/writing data: -------------------------------- */
%let in_path = ~/sasuser.v94/input_data;
%let out_path = ~/sasuser.v94/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

data public; 
 set in_lib.public2022;
 
 proc sql;
  create table subset_data as
  select
    CaseID,
    B3,
    ND2,
    ppethm,
    ppeduc5,
    GH1,
    B7_a
  from public
 quit;
```

Above I used select in proc sql to get all the needed variables I will use for my analysis.

### Part C: Get the data out of SAS and into Stata

Get the data out of SAS and into Stata. (Note that this could mean saving the data in SAS format, then importing into Stata; or exporting from SAS into Stata format then loading it in Stata; or exporting from SAS into a generic format and importing into Stata - whichever works for you.)

```         
data out_lib.public_sub;
 set public_sub;
run;
```

Above will make a new file public_sub.sas7bdat that has all the needed variables. This will have to be exported from SAS into Stata format through Stata below:

## Stata

### Part D: Demonstrate that you'be successfully extracted the appropriate data

Demonstrate that you've successfully extracted the appropriate data by showing the number of observations and variables. (Report these values via Stata code don't just say "As we see in the Properties window". The Codebook should give you a way to ensure the number of rows is as expected.)

```         
import sas using "C:\Users\heleyna\Downloads\public_sub (1).sas7bdat"
save "C:\Users\heleyna\Downloads\public_sub.dta", replace

describe

---------------------------------------------------------------------
Output:


Contains data from C:\Users\heleyna\Downloads\public_sub.dta
 Observations:        11,667                  
    Variables:             8                  23 Oct 2023 18:09
--------------------------------------------------------------------------------
Variable      Storage   Display    Value
    name         type    format    label      Variable label
--------------------------------------------------------------------------------
CaseID          int     %10.0g                CaseID 2022
B3              byte    %10.0g                Compared to 12 months ago, would
                                                you say that you (and your
                                                family) are better o
ND2             byte    %10.0g                Five years from now, do you think
                                                that the chance that you will
                                                experience a nat
ppethm          byte    %10.0g                Race / Ethnicity
ppeduc5         byte    %10.0g                Education (5 Categories)
GH1             byte    %10.0g                This section will ask some
                                                questions about your home and
                                                your car. Do you:
B7_a            byte    %10.0g                In your community - How would you
                                                rate economic conditions today:
weight_pop      double  %10.0g                Post-stratification weight - Main
                                                qualified respondents scaled to
                                                U.S. populatio
--------------------------------------------------------------------------------
Sorted by: 

. 
end of do-file
```

Above we use the describe function to show how many observations and variables we have in the data. As we can see, the number of variables and observations are what we would expect.

### Part E: Convert response to binary

The response variable is a Likert scale; convert it to a binary of worse off versus same/better.

```         
tabulate B3
gen worse_off = (B3 <=2)
tabulate worse_off
---------------------------------------------------------------------
Output:

. tabulate B3

Compared to |
  12 months |
 ago, would |
    you say |
   that you |
  (and your |
family) are |
   better o |      Freq.     Percent        Cum.
------------+-----------------------------------
          1 |      1,020        8.74        8.74
          2 |      3,276       28.08       36.82
          3 |      5,287       45.32       82.14
          4 |      1,605       13.76       95.89
          5 |        479        4.11      100.00
------------+-----------------------------------
      Total |     11,667      100.00

. 
end of do-file

. do "C:\Users\heleyna\AppData\Local\Temp\1\STD4380_000000.tmp"

. gen worse_off = (B3 <=2)

. tabulate worse_off

  worse_off |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |      7,371       63.18       63.18
          1 |      4,296       36.82      100.00
------------+-----------------------------------
      Total |     11,667      100.00
```

Above, you can observe that the inital B3 variable has 5 different categories, explained in the codebook. 1 and 2 correspond with worse off and (3-5) correspond to the same/better. I created worse_off to be a binary variable that is 0 when response variable is same/better and 1 if the response variable is worse. As we can see, the groups in B3 add up to be what we would expect for the new worse_off variable.

### Part F: Logistic regression

Use the following code to tell Stata that the data is from a complex sample:

```         
svyset CaseID [pw=weight_pop]
```

(Modify `CaseID` and `weight_pop` as appropriate if you have different variable names; those names are taken from the Codebook.)

Carry out a logisitic regression model accounting for the complex survey design. Be sure to treat variables you think should be categorical appropriately. From these results, provide an answer to the researchers question of interest.

Notice that the model does not provide a pseudo-Rsq. R has the functionality to do this.

```         
svy: logit worse_off i.ND2 i.ppethm i.ppeduc5 i.GH1 i.B7_a 

---------------------------------------------------------------------
Output:

. do "C:\Users\heleyna\AppData\Local\Temp\1\STD4380_000000.tmp"

. 
. svy: logit worse_off i.ND2 i.ppethm i.ppeduc5 i.GH1 i.B7_a 
(running logit on estimation sample)

Survey: Logistic regression

Number of strata =      1                        Number of obs   =      11,667
Number of PSUs   = 11,667                        Population size = 255,114,223
                                                 Design df       =      11,666
                                                 F(18, 11649)    =       52.04
                                                 Prob > F        =      0.0000

------------------------------------------------------------------------------
             |             Linearized
   worse_off | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         ND2 |
          2  |  -.0637524   .0913569    -0.70   0.485    -.2428273    .1153224
          3  |   .0106612   .0846977     0.13   0.900    -.1553606    .1766829
          4  |  -.2441827   .1996803    -1.22   0.221    -.6355896    .1472241
          5  |  -.2242899   .1684693    -1.33   0.183     -.554518    .1059381
             |
      ppethm |
          2  |  -.8609733   .0809155   -10.64   0.000    -1.019581   -.7023653
          3  |  -.4471861   .1193673    -3.75   0.000    -.6811659   -.2132063
          4  |  -.2682521   .0708618    -3.79   0.000    -.4071531   -.1293511
          5  |   .0975323   .1300938     0.75   0.453    -.1574733    .3525379
             |
     ppeduc5 |
          2  |   .0040619   .1016415     0.04   0.968    -.1951724    .2032963
          3  |   -.031632   .0987888    -0.32   0.749    -.2252746    .1620107
          4  |  -.1496105   .1023687    -1.46   0.144    -.3502702    .0510493
          5  |  -.0312768   .1061902    -0.29   0.768    -.2394273    .1768737
             |
         GH1 |
          2  |   .0300226   .0561748     0.53   0.593    -.0800894    .1401345
          3  |  -.1125773   .0590548    -1.91   0.057    -.2283346    .0031799
          4  |  -.3648809   .0987005    -3.70   0.000    -.5583504   -.1714115
             |
        B7_a |
          2  |  -.8141041   .0599256   -13.59   0.000    -.9315684   -.6966399
          3  |  -1.772314    .067998   -26.06   0.000    -1.905601   -1.639026
          4  |  -2.621257   .1984799   -13.21   0.000    -3.010311   -2.232204
             |
       _cons |   .6315009   .1310518     4.82   0.000     .3746174    .8883843
------------------------------------------------------------------------------

. 
end of do-file

. 
```

As we can see above, the logistic regression can give us the answer to the researcher's question. The variable that corresponds to **thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years** would be ND2. All of the p-values for each category show insignificance on the 0.05 scale. Each confidence interval includes 0 as well which indicates insignificance. Therefore, based on this analysis, we would fail to reject the null hypothesis that **the respondent's family is better off, the same, or worse off finanicially compared to 12 month's ago** cannot be predicted by **thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years**.

### Part G: Get the data out of Stata and into R

Convert .dta into .csv in Stata below:

```         
. do "C:\Users\heleyna\AppData\Local\Temp\1\STD4380_000000.tmp"

. export delimited "C:\Users\heleyna\Downloads\public_sub.csv", delimiter(",")
file C:\Users\heleyna\Downloads\public_sub.csv saved

. 
end of do-file

. 
```

## R

### Part H: Obtain the pseudo-Rsq value for the logistic model fit above and report it.

Use the survey package to obtain the pseudo-Rsq. Use the following code to set up the complex survey design:

```         
svydesign(id = ~ caseid, weight = ~ weight_pop, data = dat)
```

Obtain the pseudo-Rsq value for the logistic model fit above and report it

Read in the dataset made in stata/sas below and get a visual using head(). Also load in the survey package:

```{r}
library(survey)
library(pscl)
public_sub <- read_csv('/Users/19892/OneDrive/Documents/STATS506/ProblemSets/Stats506_PS4/public_sub.csv')
head(public_sub)
```

First, we will get the design of the survey and store it as design. Then, fit a logistic model to the data with the design and family = binomial. Print a summary of the log_mod and find the pseudo-rsq using the psrsq() function below:

```{r}
design <- svydesign(id = ~ CaseID, weight = ~ weight_pop, data = public_sub)

log_mod <- svyglm(worse_off ~ as.factor(ND2) + as.factor(ppethm) + as.factor(ppeduc5) + as.factor(GH1) + as.factor(B7_a), design = design, family = binomial)

summary(log_mod)
p_r2 <- psrsq(log_mod)
print(p_r2)
```

Above we can get the pseudo-rsq to be about 0.10676. This is fairly low, so the fit of the model is not great (want to be close to to 1).
